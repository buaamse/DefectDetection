{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "configured-exploration",
   "metadata": {},
   "source": [
    "循环神经网络RNN\n",
    "\n",
    "1. 计算图\n",
    "2. RNN\n",
    "3. 长短时记忆网络\n",
    "4. 其他RNN\n",
    "5. RNN主要应用\n",
    "\n",
    "\n",
    "\n",
    "# 计算图\n",
    "\n",
    "计算图的引入是为了后面更方便的表示网络，计算图是描述计算结构的一种图，它的元素包括节点(node)和边(edge)，节点表示变量，可以是标量、矢量、张量等，而边表示的是某个操作，即函数。\n",
    "\n",
    "![6.1](./PIC/6/6.1.png)\n",
    "\n",
    "下面这个计算图表示复合函数\n",
    "\n",
    "![6.2](./PIC/6/6.2.png)\n",
    "\n",
    "关于计算图的求导，我们可以用链式法则表示，有下面两种情况。\n",
    "\n",
    "- 情况1\n",
    "\n",
    "![6.3](./PIC/6/6.3.png)\n",
    "\n",
    "- 情况2\n",
    "\n",
    "![6.4](./PIC/6/6.4.png)\n",
    "\n",
    "求导举例：\n",
    "\n",
    "例1\n",
    "\n",
    "<img src=\"./PIC/6/6.5.png\" alt=\"6.5\" style=\"zoom:40%;\" />\n",
    "\n",
    "- a = 3, b = 1 可以得到 c = 3, d = 2, e = 6\n",
    "\n",
    "- $\\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c}\\frac{\\partial c}{\\partial a} = d = b + 1 = 2$\n",
    "- $\\frac{\\partial e}{\\partial b} = \\frac{\\partial e}{\\partial c}\\frac{\\partial c}{\\partial b}+\\frac{\\partial e}{\\partial d}\\frac{\\partial d}{\\partial b} = d + c=b+1+a+b = 5$\n",
    "\n",
    "例2\n",
    "\n",
    "<img src=\"./PIC/6/6.6.png\" alt=\"6.6\" style=\"zoom:40%;\" />\n",
    "\n",
    "$\\frac{\\partial Z}{\\partial X}=\\alpha \\delta+\\alpha \\epsilon+\\alpha \\zeta+\\beta \\delta+\\beta \\epsilon+\\beta \\zeta+\\gamma \\delta+\\gamma \\epsilon+\\gamma \\zeta = (\\alpha +\\beta+\\gamma)(\\delta+\\epsilon+\\zeta) $\n",
    "\n",
    "计算图可以很好的表示导数的前向传递和后向传递的过程，比如上面例2，前向传递了$\\frac{\\partial }{\\partial X}$ ，反向传递$\\frac{\\partial }{\\partial Z}$ 。\n",
    "\n",
    "<img src=\"./PIC/6/6.7.png\" alt=\"6.7\" style=\"zoom:30%;\" />\n",
    "\n",
    "<img src=\"./PIC/6/6.8.png\" alt=\"6.8\" style=\"zoom:30%;\" />\n",
    "\n",
    "# 循环神经网络（Recurrent Neural Network）\n",
    "\n",
    "上一章我们已经介绍了CNN，可能我们会想这里为什么还需要构建一种新的网络RNN呢？因为现实生活中存在很多序列化结构，我们需要建立一种更优秀的序列数据模型。\n",
    "\n",
    "- 文本：字母和词汇的序列\n",
    "- 语音：音节的序列\n",
    "- 视频：图像帧的序列\n",
    "- 时态数据：气象观测数据，股票交易数据、房价数据等\n",
    "\n",
    "RNN的发展历程：\n",
    "\n",
    "![6.9](/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.9.png)\n",
    "\n",
    "循环神经网络是一种人工神经网络，它的节点间的连接形成一个遵循时间序列的有向图，它的核心思想是，样本间存在顺序关系，每个样本和它之前的样本存在关联。通过神经网络在时序上的展开，我们能够找到样本之间的序列相关性。\n",
    "\n",
    "下面给出RNN的一般结构：\n",
    "\n",
    "![6.10](/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.10.png)\n",
    "\n",
    "其中各个符号的表示：$x_t,s_t,o_t$分别表示的是$t$时刻的输入、记忆和输出，$U,V,W$是RNN的连接权重，$b_s,b_o$是RNN的偏置，$\\sigma,\\varphi$是激活函数，$\\sigma$通常选tanh或sigmoid，$\\varphi$通常选用softmax。\n",
    "\n",
    "其中 softmax 函数，用于分类问题的概率计算。本质上是将一个K维的任意实数向量压缩 (映射)成另一个K维的实数向量，其中向量中的每个元素取值都介于(0，1)之间。\n",
    "$$\n",
    "\\sigma(\\vec{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}\n",
    "$$\n",
    "\n",
    "### RNN案例\n",
    "\n",
    "比如词性标注，\n",
    "\n",
    "- 我/n,爱/v购物/n,\n",
    "- 我/n在/pre华联/n购物/v\n",
    "\n",
    "Word Embedding：自然语言处理(NLP)中的 一组语言建模和特征学习技术的统称，其中来自词汇表的单词或短语被映射到实数的向量。比如这里映射到三个向量然后输入：\n",
    "\n",
    "<img src=\"./PIC/6/6.11.png\" alt=\"6.11\" style=\"zoom:30%;\" />\n",
    "\n",
    "将神经元的输出存到memory中，memory中值会作为下一时刻的输入。在最开始时刻，给定 memory初始值，然后逐次更新memory中的值。\n",
    "\n",
    "![6.12](/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.12.png)\n",
    "\n",
    "![6.13](/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.13.png)\n",
    "\n",
    "### RNN的一般结构\n",
    "\n",
    "- Elman Network\n",
    "\n",
    "<img src=\"/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.14.png\" alt=\"6.14\" style=\"zoom:50%;\" />\n",
    "\n",
    "- Jordan Network\n",
    "\n",
    "<img src=\"/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.15.png\" alt=\"6.15\" style=\"zoom:50%;\" />\n",
    "\n",
    "各种不同的RNN结构\n",
    "\n",
    "![6.16](/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.16.png)\n",
    "\n",
    "### RNN训练算法 - BPTT\n",
    "\n",
    "我们先来回顾一下BP算法，就是定义损失函数 Loss 来表示输出 $\\hat{y}$ 和真实标签 y 的误差，通过链式法则自顶向下求得 Loss 对网络权重的偏导。沿梯度的反方向更新权重的值， 直到 Loss 收敛。而这里的 BPTT 算法就是加上了时序演化，后面的两个字母 TT 就是 Through Time。\n",
    "\n",
    "<img src=\"./PIC/6/6.17.png\" alt=\"6.17\" style=\"zoom:50%;\" />\n",
    "\n",
    "我们先定义输出函数：\n",
    "$$\n",
    "\\begin{array}{l}s_{t}=\\tanh \\left(U x_{t}+W s_{t-1}\\right) \\\\ \\hat{y}_{t}=\\operatorname{softmax}\\left(V s_{t}\\right)\\end{array}\n",
    "$$\n",
    "再定义损失函数：\n",
    "$$\n",
    "\\begin{aligned} E_{t}\\left(y_{t}, \\hat{y}_{t}\\right) =-y_{t} \\log \\hat{y}_{t} \\\\ E(y, \\hat{y}) =\\sum_{t} E_{t}\\left(y_{t}, \\hat{y}_{t}\\right) \\\\ =-\\sum_{t} y_{t} \\log \\hat{y}_{t}\\end{aligned}\n",
    "$$\n",
    "<img src=\"./PIC/6/6.18.png\" alt=\"6.18\" style=\"zoom:50%;\" />\n",
    "\n",
    "我们分别求损失函数 E 对 U、V、W的梯度：\n",
    "$$\n",
    "\\begin{array}{l}\\frac{\\partial E}{\\partial V}=\\sum_{t} \\frac{\\partial E_{t}}{\\partial V} \\\\ \\frac{\\partial E}{\\partial W}=\\sum_{t} \\frac{\\partial E_{t}}{\\partial W} \\\\ \\frac{\\partial E}{\\partial U}=\\sum_{t} \\frac{\\partial E_{t}}{\\partial U}\\end{array}\n",
    "$$\n",
    "\n",
    "- 求 E 对 V 的梯度，先求 $E_3$ 对 V 的梯度\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial E_{3}}{\\partial V} &=\\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial V} \\\\ &=\\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial z_{3}} \\frac{\\partial z_{3}}{\\partial V} \\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $z_3 = V s_3$，然后求和即可。\n",
    "\n",
    "- 求 E 对 W 的梯度，先求 $E_3$ 对 W 的梯度\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\\frac{\\partial E_{3}}{\\partial W}=\\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial s_{3}} \\frac{\\partial s_{3}}{\\partial W} \\\\ s_{3}=\\tanh \\left(U x_{3}+W s_{2}\\right) \\\\ \\frac{\\partial E_{3}}{\\partial W}=\\sum_{k=0}^{3} \\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial s_{3}} \\frac{\\partial s_{3}}{\\partial s_{k}} \\frac{\\partial s_{k}}{\\partial W} \\\\ \\frac{\\partial E_{3}}{\\partial W}=\\sum_{k=0}^{3} \\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial s_{3}}\\left(\\prod_{j=k+1}^{3} \\frac{\\partial s_{j}}{\\partial s_{j-1}}\\right) \\frac{\\partial s_{k}}{\\partial W}\\end{array}\n",
    "$$\n",
    "\n",
    "其中:  $s_3$ 依赖于 $s_2$，而 $s_2$ 又依赖于 $s_1 $ 和 W ，依赖关系 一直传递到 t = 0 的时刻。因此，当我们计算对于 W 的偏导数时，不能把 $s_2$ 看作是常数项！\n",
    "\n",
    "- 求 E 对 U 的梯度，先求 $E_3$ 对 U 的梯度\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\\frac{\\partial E_{3}}{\\partial W}=\\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial s_{3}} \\frac{\\partial s_{3}}{\\partial U} \\\\ s_{3}=\\tanh \\left(U x_{3}+W s_{2}\\right) \\\\ \\frac{\\partial E_{3}}{\\partial U}=\\sum_{k=0}^{3} \\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial s_{3}} \\frac{\\partial s_{3}}{\\partial s_{k}} \\frac{\\partial s_{k}}{\\partial U}\\end{array}\n",
    "$$\n",
    "\n",
    "# 长短时记忆网络\n",
    "\n",
    "在RNN中，存在一个很重要的问题，就是梯度消失问题，一开始我们不能有效的解决长时依赖问题，其中梯度消失的原因有两个：BPTT算法和激活函数Tanh\n",
    "$$\n",
    "\\frac{\\partial E_{3}}{\\partial W}=\\sum_{k=0}^{3} \\frac{\\partial E_{3}}{\\partial \\hat{y}_{3}} \\frac{\\partial \\hat{y}_{3}}{\\partial s_{3}}\\left(\\prod_{j=k+1}^{3} \\frac{\\partial s_{j}}{\\partial s_{j-1}}\\right) \\frac{\\partial s_{k}}{\\partial W}\n",
    "$$\n",
    "有两种解决方案，分别是ReLU函数和门控RNN(LSTM).\n",
    "\n",
    "### LSTM\n",
    "\n",
    "LSTM，即长短时记忆网络，于1997年被Sepp Hochreiter 和Jürgen Schmidhuber提出来，LSTM是一种用于深度学习领域的人工循环神经网络（RNN）结构。一个LSTM单元由输入门、输出门和遗忘门组成，三个门控制信息进出单元。\n",
    "\n",
    "![6.20](./PIC/6/6.20.png)\n",
    "\n",
    "- LSTM依靠贯穿隐藏层的细胞状态实现隐藏单元之间的信息传递，其中只有少量的线性操作\n",
    "- LSTM引入了“门”机制对细胞状态信息进行添加或删除，由此实现长程记忆\n",
    "- “门”机制由一个Sigmoid激活函数层和一个向量点乘操作组成，Sigmoid层的输出控制了信息传递的比例\n",
    "\n",
    "**遗忘门**：LSTM通过遗忘门(forget gate)实现对细胞状态信息遗忘程度的控制，输出当前状态的遗忘权重，取决于 $h_{t−1}$ 和 $x_t$.\n",
    "$$\n",
    "f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)\n",
    "$$\n",
    "<img src=\"./PIC/6/6.21.png\" alt=\"6.21\" style=\"zoom:50%;\" />\n",
    "\n",
    "**输入门**：LSTM通过输入门(input gate)实现对细胞状态输入接收程度的控制，输出当前输入信息的接受权重，取决于 $h_{t−1}$ 和 $x_t$.\n",
    "$$\n",
    "\\begin{array}{c}i_{t}=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\ \\tilde{C}_{t}=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right)\\end{array}\n",
    "$$\n",
    "<img src=\"/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.22.png\" alt=\"6.22\" style=\"zoom:50%;\" />\n",
    "\n",
    "**输出门**：LSTM通过输出门(output gate)实现对细胞状态输出认可程度的控制，输出当前输出信息的认可权重，取决于 $h_{t−1}$ 和 $x_t$.\n",
    "$$\n",
    "o_{t}=\\sigma\\left(W_{o} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right)\n",
    "$$\n",
    "<img src=\"./PIC/6/6.23.png\" alt=\"6.23\" style=\"zoom:50%;\" />\n",
    "\n",
    "**状态更新**：“门”机制对细胞状态信息进行添加或删除，由此实现长程记忆。\n",
    "$$\n",
    "\\begin{array}{c}C_{t}=f_{t} * C_{t-1}+i_{t} * \\tilde{C}_{t} \\\\ h_{t}=o_{t} * \\tanh \\left(C_{t}\\right)\\end{array}\n",
    "$$\n",
    "<img src=\"./PIC/6/6.24.png\" alt=\"6.24\" style=\"zoom:50%;\" />\n",
    "\n",
    "下面给出一个标准化的RNN例子\n",
    "\n",
    "```python\n",
    "#构造RNN网络，x的维度5，隐层的维度10,网络的层数2\n",
    "rnn_ seq = nn.RNN(5， 10,2)\n",
    "#构造一个输入序列，长为6，batch是3，特征是5\n",
    "X =V(torch. randn(6， 3，5))\n",
    "#out,ht = rnn_ seq(x， h0) # h0可以指定或者不指定\n",
    "out,ht = rnn_ seq(x)\n",
    "# q1:这里out、ht的size是多少呢? out:6*3*10， ht:2*3*10\n",
    "\n",
    "#输入维度50，隐层100维，两层\n",
    "Lstm_ seq = nn.LSTM(50， 100，num layers=2 )\n",
    "#输入序列seq= 10，batch =3，输入维度=50\n",
    "lstm input = torch. randn(10，3，50)\n",
    "out, (h, c) = lstm_ seq(lstm_ _input) #使用默认的全0隐藏状态\n",
    "```\n",
    "\n",
    "# 其他经典的循环神经网络\n",
    "\n",
    "### Gated Recurrent Unit(GRU)\n",
    "\n",
    "Gated Recurrent Unit (GRU)，是在2014年提出的，可认为是LSTM 的变种，它的细胞状态与隐状态合并，在计算当前时刻新信息的方法和LSTM有 所不同；GRU只包含重置门和更新门；在音乐建模与语音信号建模领域与LSTM具有相似的性能，但是参数更少，只有两个门控。\n",
    "\n",
    "![6.19](./PIC/6/6.19.png)\n",
    "\n",
    "### Peephole LSTM\n",
    "\n",
    "让门层也接受细胞状态的输入，同时考虑隐层信息的输入。\n",
    "\n",
    "<img src=\"./PIC/6/6.25.png\" alt=\"6.25\" style=\"zoom:50%;\" />\n",
    "\n",
    "### Bi-directional RNN(双向RNN) \n",
    "\n",
    "Bi-directional RNN(双向RNN)假设当前t的输出不仅仅和之前的序列有关，并且还与之后的序列有关，例如：完形填空，它由两个RNNs上下叠加在一起组成，输出由这两个RNNs的隐藏层的状态决定。\n",
    "\n",
    "<img src=\"./PIC/6/6.26.png\" alt=\"6.26\" style=\"zoom:50%;\" />\n",
    "\n",
    "<img src=\"./PIC/6/6.27.png\" alt=\"6.27\" style=\"zoom:50%;\" />\n",
    "\n",
    "### Continuous time RNN(CTRNN)\n",
    "\n",
    "CTRNN利用常微分方程系统对输入脉冲序列神经元的影响 进行建模。CTRNN被应用到进化机器人中，用于解决视觉、协作和最 小认知行为等问题。\n",
    "\n",
    "<img src=\"./PIC/6/6.28.png\" alt=\"6.28\" style=\"zoom:50%;\" />\n",
    "\n",
    "# 循环神经网络的主要应用\n",
    "\n",
    "### 语言模型\n",
    "\n",
    "根据之前和当前词预测下一个单词或者字母\n",
    "\n",
    "<img src=\"./PIC/6/6.29.png\" alt=\"6.29\" style=\"zoom:50%;\" />\n",
    "\n",
    "问答系统\n",
    "\n",
    "<img src=\"/Users/liuyang/Desktop/中科院/datawhale/DL理论/PIC/6/6.30.png\" alt=\"6.30\" style=\"zoom:50%;\" />\n",
    "\n",
    "### 自动作曲\n",
    "\n",
    "<img src=\"./PIC/6/6.31.png\" alt=\"6.31\" style=\"zoom:50%;\" />\n",
    "\n",
    "参考：Hang Chu, Raquel Urtasun, Sanja Fidler. Song From PI: A Musically Plausible Network for Pop Music Generation. CoRR abs/1611.03477 (2016)\n",
    "\n",
    "Music AI Lab: **https://musicai.citi.sinica.edu.tw/**\n",
    "\n",
    "<img src=\"./PIC/6/6.32.png\" alt=\"6.32\" style=\"zoom:50%;\" />\n",
    "\n",
    "### 机器翻译\n",
    "\n",
    "将一种语言自动翻译成另一种语言\n",
    "\n",
    "<img src=\"./PIC/6/6.35.png\" alt=\"6.35\" style=\"zoom:50%;\" />\n",
    "\n",
    "### 自动写作\n",
    "\n",
    "根据现有资料自动写作，当前主要包括新闻写作和诗歌创作。主要是基于RNN&LSTM的文本生成技术来实现，需要训练大量同 类文本，结合模板技术。\n",
    "\n",
    "目前主要产品有：腾讯Dreamwriter写稿机器人、今日头条xiaomingbot、第一财经DT稿王(背后是阿里巴巴) 、百度Writing-bots...\n",
    "\n",
    "### 图像描述\n",
    "\n",
    "根据图像形成语言描述\n",
    "\n",
    "<img src=\"./PIC/6/6.33.png\" alt=\"6.33\" style=\"zoom:50%;\" />\n",
    "\n",
    "<img src=\"./PIC/6/6.34.png\" alt=\"6.34\" style=\"zoom:50%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-engine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
